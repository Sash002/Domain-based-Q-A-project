# Domain-based-Q-A-project

Task 1: Given a question and a set of paragraphs, predict if the question can be answered with the given paragraphs. If yes, return the paragraph that answers the question. Each question and paragraph is associated with a specific theme. This could be “Sports”, “English” or “Mathematics” etc. A theme question can be answered by one of the paragraphs in that theme. 

Task 2: For the given questions, also predict the exact answer from the predicted paragraph. Predict the start_index and the answer_text field for the given question. Note: Both tasks will be marked individually. However, to perform better in Task 2, your model needs to perform better in Task 1.

Our approach till now has been quite restricted due to the hardware and time constraints. However, we still have managed to try out a few different things. So far, we have divided our attempts into three parts:
1.	Temporarily ignoring hardware constraints- 
We started off with researching about the task at hand, we found that the Stanford Question Answering Dataset (SQuADv2) dataset is widely used as a benchmark for Question-Answering tasks and is quite similar to the given data. We found a research paper that had a high F1 score on SQuADv2, Retrospective Reader for Machine Reading Comprehension (Zhang et al. 2021). We implemented the method and had promising results on the given training dataset with an F1 score of around 0.89 using ELECTRA as the backbone. Retro Reader works well because it imitates the human way of text comprehension, by roughly looking for answers in the given passage and then actually finding out the answer. 
The issue with this method was that it was very hardware intensive requiring 4 hours to train on a colab GPU. Due to the accelerator constraint we had to drop this idea.
2.	Classical ML methods with lower computational budget training
We moved on to using Bi-directional LSTMs taking inspiration from a master’s project from Santa Carla University. The method involved training two bidirectional LSTMs for extraction of embeddings and then computing a cosine similarity between them to ascertain whether the question is answerable or not. We tried training this model using only the given constraints i.e. CPU but training took over 30 hours. We then proceeded to use GPU to train it to completion but the F1 score was far too low to be considered. Thus we had to give this method up as well.

3.	Using Pretrained Models on SQuADv2 dataset
Our current method is to use a model pretrained on the SQuADv2 dataset. This dataset is very similar to the given training set, in that it has questions which have to be answered given a context. Considering that we have significantly lesser data than in SQuADv2 and the hardware constraints, we decided not to fine-tune the pretrained model on our dataset. On evaluating 8 backbones trained on SQuADv2 using the metric that we will be judged on we decided to use tiny- RoBERTa as the final model to be used. We found that larger models had a much larger inference time and although had better F1 scores their average inference times offset their final metric score so that they would perform worse. This is why we only tested light weight models for inference of the test set. We created the test set using Group K-Fold method which is further explained in the next section.
